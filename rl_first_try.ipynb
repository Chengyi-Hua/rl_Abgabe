{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notbook for refinforcement learning assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entwickle und optimiere einen RL-Agenten f√ºr diese Umgebung in Python, die Gestaltung ist dabei ganz Dir √ºberlassen. Beschreibe Deine Vorgehensweise u.a. hinsichtlich Wahl der Architektur, Training und Optimierung. Erkl√§re angewandte Konzepte und begr√ºnde Deine Entscheidungen.\\ üìì Entwickle Deinen RL-Agenten in einem Jupyter Notebook (ggf. in Google Colab). Dokumentiere mithilfe von inline-comments und/oder Markdown-Cells, sowie einer ReadMe.md und requirements.txt zur Ausf√ºhrung des Codes. F√ºhre den Code vor Commit einmal komplett aus, damit die Outputs der einzelnen Zellen im Notebook auf Github sichtbar sind. <br>\n",
    "ü§ñ Der finale RL-Agent muss nicht perfekt sein, er sollte sich aber im Laufe des Trainings jedoch verbessern und eine akzeptable Performance aufweisen.<br>\n",
    "üöø Clean code: Gehe sicher, dass Dein Code klar, verst√§ndlich und wartbar ist, sowie den pep8-Standard erf√ºllt.<br>\n",
    "üí° Mache Deine Entscheidungen auch hier transparent durch eine vollst√§ndige, nachvollziehbare und logische Ausf√ºhrung Deiner Vorgehensweise mit Bezug auf theoretische Konzepte. <br>\n",
    "üìö Nochmal zur Betonung: Du darfst Code wiederverwenden, musst dies aber kennzeichnen! Es gibt keinen Abzug f√ºr korrekt gekennzeichneten Code aus externen Quellen.<br> ‚öñÔ∏è Bei Gruppenarbeit sollte zumindest eine Optimierungstechnik f√ºr den finalen RL-Agenten angewandt worden sein, wie z.B. ein recherchiertes oder gelerntes Konzept aus der Vorlesung oder mit Hyperparametertuning. Auch das geh√∂rt in den Projektbericht. Zus√§tzlich muss die ReadMe.md eine detaillierte √úbersicht enthalten, wie die Aufgaben auf Gruppenmitglieder verteilt wurden.<br>\n",
    "<br>\n",
    "### Arbeitsweise: Einzelarbeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_10828\\2275976978.py:5: RuntimeWarning: overflow encountered in subtract\n",
      "  discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / np.array(DISCRETE_OS_SIZE).reshape(-1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der diskreten Zust√§nde f√ºr jeden Zustandsraum\n",
    "DISCRETE_OS_SIZE = [20] * len(env.observation_space.high)\n",
    "\n",
    "# Gr√∂√üe des Zustandsraums\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / np.array(DISCRETE_OS_SIZE).reshape(-1, 1)\n",
    "\n",
    "# Initialisierung der Q-Tabelle mit zuf√§lligen Werten\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [env.action_space.n]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n",
    "    return tuple(discrete_state.astype(int))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_10828\\2427837142.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  discrete_state = (state - env.observation_space.low) / discrete_os_win_size\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m epsilon_decay_value \u001b[39m=\u001b[39m epsilon \u001b[39m/\u001b[39m (END_EPSILON_DECAYING \u001b[39m-\u001b[39m START_EPSILON_DECAYING)\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPISODES):\n\u001b[1;32m---> 13\u001b[0m     discrete_state \u001b[39m=\u001b[39m get_discrete_state(env\u001b[39m.\u001b[39;49mreset())\n\u001b[0;32m     14\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     17\u001b[0m         \u001b[39m# Exploration vs. Exploitation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m, in \u001b[0;36mget_discrete_state\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_discrete_state\u001b[39m(state):\n\u001b[1;32m----> 2\u001b[0m     discrete_state \u001b[39m=\u001b[39m (state \u001b[39m-\u001b[39;49m env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mlow) \u001b[39m/\u001b[39m discrete_os_win_size\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(discrete_state\u001b[39m.\u001b[39mastype(\u001b[39mint\u001b[39m))\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (4,) "
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 10000\n",
    "\n",
    "# Exploration und Exploitation\n",
    "epsilon = 1\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES // 2\n",
    "epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Exploration vs. Exploitation\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "\n",
    "        # Q-Wert aktualisieren\n",
    "        if not done:\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        elif new_state[0] >= env.goal_position:\n",
    "            q_table[discrete_state + (action,)] = 0\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "\n",
    "    # Epsilon reduzieren (exploration vs. exploitation)\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_decay_value\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np \n",
    "import time, math, random\n",
    "from typing import Tuple\n",
    "\n",
    "# import gym \n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m obs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m80\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     actions \u001b[39m=\u001b[39m policy(\u001b[39m*\u001b[39;49mobs)\n\u001b[0;32m      7\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions) \n\u001b[0;32m      8\u001b[0m     env\u001b[39m.\u001b[39mrender()\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "policy = lambda obs: 1\n",
    "\n",
    "for _ in range(5):\n",
    "    obs = env.reset()\n",
    "    for _ in range(80):\n",
    "        actions = policy(*obs)\n",
    "        obs, reward, done, info = env.step(actions) \n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
