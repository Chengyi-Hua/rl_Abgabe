{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep Q Network (DQN) class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate (start high for exploration)\n",
    "        self.epsilon_decay = 0.995  # Decay rate of exploration rate\n",
    "        self.epsilon_min = 0.01  # Minimum exploration rate\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the DQN agent on the CartPole environment\n",
    "def train_cartpole_dqn(episodes=1000, batch_size=32):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    # Variables for capturing frames\n",
    "    frames = []  # To store the frames from the environment\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        time = 0\n",
    "\n",
    "        while not done:\n",
    "            # Capturing frames\n",
    "            frames.append(env.render(mode='rgb_array'))\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done or time == 499 else -100\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            time += 1\n",
    "\n",
    "            if done:\n",
    "                # Display progress based on the score and epsilon\n",
    "                print(f\"Episode {episode}/{episodes}, Score: {time}, Epsilon: {agent.epsilon}\")\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "        # Display frames as animation after every few episodes\n",
    "        if episode % 50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            fig = plt.figure()\n",
    "            ims = [[plt.imshow(frame, animated=True)] for frame in frames]\n",
    "            ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True)\n",
    "            plt.close()  # Close the figure to prevent duplicate display\n",
    "            display(ani)\n",
    "            frames = []  # Reset frames after displaying\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.ArtistAnimation at 0x15294a437c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Score: 28, Epsilon: 1.0\n",
      "Episode 2/1000, Score: 10, Epsilon: 0.995\n",
      "Episode 3/1000, Score: 21, Epsilon: 0.990025\n",
      "Episode 4/1000, Score: 19, Epsilon: 0.985074875\n",
      "Episode 5/1000, Score: 12, Epsilon: 0.9801495006250001\n",
      "Episode 6/1000, Score: 16, Epsilon: 0.9752487531218751\n",
      "Episode 7/1000, Score: 10, Epsilon: 0.9703725093562657\n",
      "Episode 8/1000, Score: 18, Epsilon: 0.9655206468094844\n",
      "Episode 9/1000, Score: 13, Epsilon: 0.960693043575437\n",
      "Episode 10/1000, Score: 12, Epsilon: 0.9558895783575597\n",
      "Episode 11/1000, Score: 32, Epsilon: 0.9511101304657719\n",
      "Episode 12/1000, Score: 16, Epsilon: 0.946354579813443\n",
      "Episode 13/1000, Score: 34, Epsilon: 0.9416228069143757\n",
      "Episode 14/1000, Score: 24, Epsilon: 0.9369146928798039\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_cartpole_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000, Score: 19, Epsilon: 0.5\n",
      "Episode 1/1000, Score: 14, Epsilon: 0.5\n",
      "Episode 2/1000, Score: 12, Epsilon: 0.4975\n",
      "Episode 3/1000, Score: 12, Epsilon: 0.4950125\n",
      "Episode 4/1000, Score: 23, Epsilon: 0.4925374375\n",
      "Episode 5/1000, Score: 50, Epsilon: 0.49007475031250003\n",
      "Episode 6/1000, Score: 119, Epsilon: 0.48762437656093754\n",
      "Episode 7/1000, Score: 46, Epsilon: 0.48518625467813287\n",
      "Episode 8/1000, Score: 32, Epsilon: 0.4827603234047422\n",
      "Episode 9/1000, Score: 40, Epsilon: 0.4803465217877185\n",
      "Episode 10/1000, Score: 22, Epsilon: 0.47794478917877986\n",
      "Episode 11/1000, Score: 32, Epsilon: 0.47555506523288593\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     train_cartpole_dqn()\n",
      "Cell \u001b[1;32mIn[1], line 83\u001b[0m, in \u001b[0;36mtrain_cartpole_dqn\u001b[1;34m(episodes, batch_size)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m batch_size:\n\u001b[1;32m---> 83\u001b[0m         agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n\u001b[0;32m     85\u001b[0m env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m     87\u001b[0m \u001b[39m# Plotting the scores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m     46\u001b[0m     target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(next_state)[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 47\u001b[0m target_f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[0;32m     48\u001b[0m target_f[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n\u001b[0;32m     49\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(state, target_f, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1696\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1690\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1692\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1693\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1694\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 1696\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1697\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1698\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1699\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   1700\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   1701\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1702\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1703\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1704\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1705\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1706\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1708\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1364\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1363\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1364\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1152\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1149\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1150\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_value \u001b[39m=\u001b[39m steps_per_execution\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m-> 1152\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m   1153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verify_data_adapter_compatibility(adapter_cls)\n\u001b[0;32m   1154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1155\u001b[0m     x,\n\u001b[0;32m   1156\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     distribution_strategy\u001b[39m=\u001b[39mds_context\u001b[39m.\u001b[39mget_strategy(),\n\u001b[0;32m   1166\u001b[0m     model\u001b[39m=\u001b[39mmodel)\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:988\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_data_adapter\u001b[39m(x, y):\n\u001b[0;32m    987\u001b[0m   \u001b[39m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 988\u001b[0m   adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m    989\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    990\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    994\u001b[0m             _type_name(x), _type_name(y)))\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:988\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_data_adapter\u001b[39m(x, y):\n\u001b[0;32m    987\u001b[0m   \u001b[39m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 988\u001b[0m   adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcan_handle(x, y)]\n\u001b[0;32m    989\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m    990\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    992\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    994\u001b[0m             _type_name(x), _type_name(y)))\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:227\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.can_handle\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m   flat_inputs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mflatten(y)\n\u001b[1;32m--> 227\u001b[0m tensor_types \u001b[39m=\u001b[39m _get_tensor_types()\n\u001b[0;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_tensor\u001b[39m(v):\n\u001b[0;32m    230\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, tensor_types):\n",
      "File \u001b[1;32mc:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1635\u001b[0m, in \u001b[0;36m_get_tensor_types\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1633\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_tensor_types\u001b[39m():\n\u001b[0;32m   1634\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1635\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m  \u001b[39m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m   1637\u001b[0m     \u001b[39mreturn\u001b[39;00m (ops\u001b[39m.\u001b[39mTensor, np\u001b[39m.\u001b[39mndarray, pd\u001b[39m.\u001b[39mSeries, pd\u001b[39m.\u001b[39mDataFrame)\n\u001b[0;32m   1638\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:786\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:881\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:979\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Define the Deep Q Network (DQN) class\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.9  # Lower discount factor for short-sighted agent\n",
    "        self.epsilon = 0.5  # Higher exploration rate for more random actions\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001  # Higher learning rate for faster but unstable learning\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(24, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dense(24, activation='relu'),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Training the DQN agent on the CartPole environment\n",
    "def train_cartpole_dqn(episodes=1000, batch_size=32):\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    scores = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        done = False\n",
    "        time = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward = reward if not done or time == 499 else -100\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            time += 1\n",
    "\n",
    "            if done:\n",
    "                scores.append(time)\n",
    "                print(f\"Episode {episode}/{episodes}, Score: {time}, Epsilon: {agent.epsilon}\")\n",
    "                break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Plotting the scores\n",
    "    plt.plot(scores)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('DQN Agent Performance')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_cartpole_dqn()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
