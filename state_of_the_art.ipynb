{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-art Implementation \n",
    "In meiner Recherche bin ich den Blog: \"CartPole Balance using Python | OpenAI Gym | Reinforcement Learning Project Tutorial\" gestoßen. UNd würde es im Rahmen des Projektes ausprobieren. Aus dem einfachen Grund, dass meine Implementation mit Q-Net beziehungsweise Dueling Q-Net als Optimierung keine befriegigende Leistung geliefert hat. Es ist sehr Interessant zu sehen, wie bereits existierende Lösungen fnktionieren. Ich würde diesen Ansatz als Alternative ansehen und versuchen zu verstehen was sein Gedankengang war.\n",
    "\n",
    "Code von<br> \n",
    "Aswintechguy, <br>\n",
    "Titel: CartPole Balance using Python | OpenAI Gym | Reinforcement Learning Project Tutorial <br>\n",
    "URL: https://www.hackersrealm.net/post/cartpole-balance-reinforcement-learning <br>\n",
    "Zugriffsdatum: 20.07.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install modules\n",
    "# !pip install gym stable_baselinese\n",
    "\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt folgt eine einfache Trainingsschleife, in der der Agent zehn Episoden lang trainiert wird. Für jede Episode wird die Umgebung zurückgesetzt, und der Agent interagiert mit der Umgebung, bis die Episode abgeschlossen ist. Der Agent wählt bei jedem Schritt zufällige Aktionen (env.action_space.sample()), und die Umgebung gibt den nächsten Zustand (n_state), die Belohnung der Aktion und eine Flagge, die angibt, ob die Episode beendet ist (done), zurück. Die gesamte Belohnung der Episode wird in der Variable score gesammelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 Score: 13.0\n",
      "Episode: 2 Score: 16.0\n",
      "Episode: 3 Score: 22.0\n",
      "Episode: 4 Score: 10.0\n",
      "Episode: 5 Score: 42.0\n",
      "Episode: 6 Score: 14.0\n",
      "Episode: 7 Score: 54.0\n",
      "Episode: 8 Score: 22.0\n",
      "Episode: 9 Score: 17.0\n",
      "Episode: 10 Score: 20.0\n"
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "\n",
    "for episode in range(1, 11):\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "    print('Episode:', episode, 'Score:', score)\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier wird eine neue Instanz der Umgebung erstellt und in eine DummyVecEnv gehüllt. \n",
    "# Dies ist hilfreich, um mehrere Umgebungen parallel zu trainieren. \n",
    "# In diesem Code wird jedoch nur eine Umgebung verwendet.\n",
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda: env])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt wird ein PPO Modell definiert.<br>\n",
    "Proximal Policy Optimization, oder PPO, ist ein on-policy Reinforcement-Learning-Algorithmus, der aufgrund seiner Effektivität und Stabilität beliebt geworden ist. Das Ziel besteht darin, die optimale Richtlinie zu finden, indem die Richtlinienparameter iterativ aktualisiert werden. Er gehört zur Familie der Policy-Gradient-Methoden und verwendet eine Ersatzzielfunktion, um die Richtlinienaktualisierung zu approximieren. Ein wichtiger Aspekt von PPO ist die Verwendung von abgeschnittenen Wahrscheinlichkeitsverhältnissen, um zu verhindern, dass die Richtlinienaktualisierungen zu groß werden und das Training destabilisieren. Das Ziel besteht darin, das Wahrscheinlichkeitsverhältnis der neuen Richtlinie im Vergleich zur alten Richtlinie zu maximieren, wird jedoch innerhalb eines bestimmten Bereichs abgeschnitten, um große Änderungen zu verhindern.\n",
    "\n",
    "\n",
    "Vorteile:<br>\n",
    "- Stabilität: PPO ist bekannt für seine Stabilität im Training von RL-Agenten. Es verwendet gekappte Wahrscheinlichkeitsverhältnisse, um zu verhindern, dass die Richtlinienaktualisierungen zu groß werden, was zu einem effizienten und konsistenten Lernen führt.\n",
    "- On-Policy: PPO ist ein On-Policy-Algorithmus, was bedeutet, dass er die Richtlinie während des Trainings aktualisiert, indem er Daten von der aktuellen Richtlinie sammelt. Dies kann dazu beitragen, Probleme mit nicht-stationären Umgebungen zu vermeiden.\n",
    "- Effiziente Nutzung von Daten: PPO verwendet Clipping und einen K-Lipschitz-Parameter, um die Lernrate zu begrenzen. Dadurch werden die Daten effizienter genutzt und das Lernen verbessert.\n",
    "\n",
    "Nachteile:<br>\n",
    "- Exploration vs. Exploitation: Obwohl PPO eine gewisse Exploration ermöglicht, kann es manchmal eine Balance zwischen Exploration und Exploitation schwierig sein. Die Balance kann durch die Wahl geeigneter Hyperparameter erreicht werden, aber das Tuning kann zeitaufwändig sein.\n",
    "- Hyperparameter-Tuning: PPO hat mehrere Hyperparameter, die sorgfältig abgestimmt werden müssen, um optimale Ergebnisse zu erzielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1097 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 846          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065650987 |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.687       |\n",
      "|    explained_variance   | -0.0127      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.58         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00781     |\n",
      "|    value_loss           | 54.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 711         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009204378 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0685      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    value_loss           | 43.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008921553 |\n",
      "|    clip_fraction        | 0.0915      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.211       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 53.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 700         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009508394 |\n",
      "|    clip_fraction        | 0.0694      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 63.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 693        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00487108 |\n",
      "|    clip_fraction        | 0.0289     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.603     |\n",
      "|    explained_variance   | 0.313      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 23.4       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.00853   |\n",
      "|    value_loss           | 68.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 689         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005835175 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.444       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44.6        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00912    |\n",
      "|    value_loss           | 54.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 688          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038871993 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.534        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00805     |\n",
      "|    value_loss           | 61           |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 685        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01223215 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.572     |\n",
      "|    explained_variance   | 0.836      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.32       |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00926   |\n",
      "|    value_loss           | 31.9       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 683          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050150678 |\n",
      "|    clip_fraction        | 0.0482       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.572       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.81         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00948     |\n",
      "|    value_loss           | 35.1         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x235aa4adaf0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modell erstellen und trainieren\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484.4, 25.2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erklärung der Werte\n",
    "\n",
    "484.4:\n",
    "- Dieser Wert ist der durchschnittliche Gesamtscore, den der Agent über die 10 Bewertungsepisoden erzielt hat. Der Gesamtscore ist die Gesamtbelohnung, die der Agent in einer Episode gesammelt hat. Ein höherer Durchschnittsscore zeigt an, dass der Agent in der Umgebung eine bessere Leistung erbracht hat. In diesem Fall erzielte der Agent einen durchschnittlichen Gesamtscore von etwa 484.4 über die Bewertungsepisoden.\n",
    "\n",
    "25.2:\n",
    "- Dieser Wert ist die Standardabweichung des Gesamtscores über die 10 Bewertungsepisoden. Die Standardabweichung misst die Streuung der Werte um den Durchschnitt. Eine niedrige Standardabweichung deutet darauf hin, dass die Ergebnisse in den Bewertungsepisoden näher beieinander liegen, was auf eine konsistente Leistung des Agenten hindeutet. In diesem Fall beträgt die Standardabweichung etwa 25.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigene Implementation: Erweiterte oder tiefere Einblicke in die Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Funktion zum Visualisieren des Agenten definieren\n",
    "def visualize_agent(agent, env, episodes=5):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action, _ = agent.predict(state)  # Aktion vorhersagen\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Visualisierung des Agenten über 5 Episoden\n",
    "visualize_agent(model, env, episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
